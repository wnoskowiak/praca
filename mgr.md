# Liquid Argon detectors

## Microboone experiment

### MicroBooNE OpenSamples Dataset

- przedyskutować co znajduje się w samych datasetach z microboone-a
- podać tabelkę w której opisane jest ile eventów w jakim pliku

The MicroBooNE OpenSamples dataset comprises two publicly available samples designed to facilitate research and development in neutrino physics and machine learning applications. Both samples feature simulated neutrino interactions from the Booster Neutrino Beam (BNB) overlaid onto real cosmic ray background data collected by the MicroBooNE detector. The first sample is inclusive, encompassing all neutrino flavors and interaction types occurring throughout the entire cryostat volume, with event distributions reflecting the experiment's nominal flux and cross-section models. The second sample focuses specifically on charged-current electron neutrino interactions within the active volume of the liquid argon time projection chamber (LArTPC). The datasets are provided in two formats: HDF5 and art/ROOT.

In further discussions we will focus only on the files containing wire info.

---

## Envirement preperation

The Jupyter notebooks accompanying the MicroBooNE OpenSamples repository recommend creating a Conda environment that includes the necessary packages. While there is nothing inherently wrong with this approach, it does introduce several disadvantages. Beyond general inefficiencies—such as longer environment loading times, higher memory usage, and increased disk space consumption—the most significant issue encountered was related to the sheer number of files generated by Conda distributions.

Both Conda and Miniconda installations can create and maintain hundreds of thousands of files. Although the total disk usage of these files is typically not problematic (many are symbolic links or small metadata files), the system used for processing data enforced strict quotas on the number of files each user could create. Installing a full Conda distribution brought the user account dangerously close to this limit, which severely impacted the usability of the system. For instance, launching a new Jupyter session became almost impossible, as nearly every action related to managing Jupyter generates additional log files, further pushing against the file quota.

Additionally, the environment setup proposed in the MicroBooNE OpenSamples notebooks depended on outdated libraries—most notably pynuml version 0.3, which is no longer maintained and is poorly documented. This package also requires an outdated version of Python (3.7) and depends on a deprecated version of HDF5. These constraints make it effectively impossible to use current versions of many essential packages, which significantly complicates the development process.

To address these issues and improve the portability and maintainability of the tools used for processing OpenSamples data, an alternative approach to environment management was adopted.

---

## Containarization and singularity

Containerization is a method of packaging software and its dependencies into isolated, self-contained units called containers. Each container includes all necessary binaries, libraries, and configuration files, ensuring that applications run consistently across different systems regardless of variations in the underlying infrastructure. Modern containerization solutions allow for building, distributing, and executing containers in a lightweight and reproducible manner. Containers are typically defined using a configuration file that builds on a base image and specifies additional dependencies and setup steps, producing a new image that can be versioned and reused.

In this specific case, a containerization tool called Singularity was used. Singularity is particularly well-suited for high-performance computing environments, as it enables users to define, build, and execute containers without requiring root privileges. A container definition file was created based on a modern Ubuntu base image. This definition specified the installation of all required dependencies and packages during the container build process. Using this approach made it possible to work with up-to-date software versions, including Python 3.9, the latest maintained release of the pynuml package, and a current version of HDF5, although these upgrades requires some code refactorings to be performed on the code present in open samples notebooks. Moreover, since the resulting container image is stored as a single file, this method also effectively circumvented the file quota limitations encountered with Conda-based setups.

---

## Image rendering

- script provided by microboone opensamples was used and modified (describe how these images were generated exactly - steps and all)
- images generated in greysacle, scaled down 2x for conviniece
- events were matched with data from the "xxxx" that included "..."
- combined into .csv manifest files

### Final image format

- how many pngs
- how many gbs

---

## Events present in the dataset

- rozpisać dokładną dyskusję tego jakie eventy znajdują się w datasecie - skutecznie przeanalizować tabelki w plikach csv z przeprocesowanych danych. histogram energii, ile neutrin mionowych, ile elekronowych, podział na charged current i non charged current

---

## Further dataset processing

Although the final image dataset occupied approximately 13 GB on disk, this was primarily due to the use of compressed image formats (specifically, .png). When these images were loaded into memory as arrays of integers, their size increased dramatically—by more than a factor of 100 in some cases. It quickly became evident that the dataset was too large to be feasibly loaded into the system’s RAM without causing memory fragmentation. However, loading each image individually into memory incurred significant computational overhead: each image had to be read from disk, memory had to be allocated, and the image had to be decompressed before being converted into an array. Since training a convolutional neural network (CNN) typically requires each image to be loaded multiple times, it was crucial to minimize the number of image loading operations.

{Insert a chart comparing the total size of compressed images with the size of the same images loaded into memory as dense arrays.}

During development, it was observed that the processed images contained mostly empty (zero-value) pixels. While traces were present in all images, they typically occupied only a small portion of each image—approximately 5% of the total pixels. (Note: Mention that the threshold used during the generation of wire images was sufficiently low to eliminate noise.) As a result, the decision was made to represent the images using sparse arrays. Unlike dense arrays, where each element is stored explicitly in memory, sparse arrays assume a default value for all positions and store only the non-zero values along with their coordinates. Although these sparse arrays still need to be converted to dense format before being passed to the CNN, this conversion is significantly less costly than loading and decompressing images from disk. This optimization led to a substantial reduction in memory usage—by up to a factor of 70—making it feasible to load the entire dataset into RAM.

{Insert a chart comparing the total size of compressed images with the total memory usage of the sparse array representation.}

Another challenge arose from the number of individual files comprising the dataset. The final dataset contained approximately {(verify this number) 120,000} image files. This created issues similar to those previously encountered when working with Miniconda—namely, the system used for CNN training imposed quotas on the number of files each user could allocate. Operating under an account close to that limit proved to be cumbersome. Notably, the total dataset size remained well below the storage quota in terms of bytes, indicating that reducing the number of files could resolve the problem without affecting overall storage usage.

To address both issues, each .csv manifest file was loaded into a pandas DataFrame. Three additional columns were added to each record—one for each image associated with an event—and populated with the corresponding sparse arrays. Each resulting dataset was then saved as a single .pkl file, a format that preserves Python objects as they exist in memory. The combined size of all .pkl files was approximately {(verify this number) 32 GB}, which corresponds closely to the amount of memory required to load them into RAM.

---

## Convolutional neural networks

- ogólny opis CNNów - czym są, jak działają
- dyskusja dla czego mogą one być w tym przypadku użyteczne

---

### Neural net - application in specific case

- training prep
- describe networks used
- describe label
- describe data
- "settings"